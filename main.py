# main.py
import os
import re
import json
import requests
import subprocess
import sys
import tempfile
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from recipe_scrapers import scrape_me
import yt_dlp
from openai import OpenAI

# DO NOT auto-update yt-dlp - newer versions break with Python 3.11
# Pinned to 2024.10.22 which is stable

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Enhanced user agents for bot detection avoidance
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
]

# Configuration from environment
YT_DLP_SLEEP_INTERVAL = int(os.getenv("YT_DLP_SLEEP_INTERVAL", "2"))
YT_DLP_MAX_SLEEP_INTERVAL = int(os.getenv("YT_DLP_MAX_SLEEP_INTERVAL", "5"))
DEBUG_MODE = os.getenv("DEBUG_MODE", "false").lower() == "true"

# ONLY CHANGE: spaces replaced with real TABs (yt-dlp requires this)
INSTAGRAM_COOKIES = """# Netscape HTTP Cookie File
# This file is generated by yt-dlp. Do not edit.
.instagram.com	TRUE	/	FALSE	1733875200	csrftoken	abxvXW3Nl1NZES5GKhSebmYt7chBhJcK
.instagram.com	TRUE	/	FALSE	1729999569	datr	raTLaBySXpCKW2Tm0ctSbzzO
.instagram.com	TRUE	/	FALSE	1731282769	ds_user_id	46425022
.instagram.com	TRUE	/	FALSE	1728789640	ig_did	8A6D83CE-01C7-4F7B-932A-4E1B53BDD872
.instagram.com	TRUE	/	FALSE	1728789631	ig_nrcb	1
.instagram.com	TRUE	/	FALSE	1730074964	mid	aM2o1AAEAAGpGRetOSmne116jpAx
.instagram.com	TRUE	/	FALSE	1765735567	rur	NHA\\05446425022\\0541794209167:01fe194320b30b02ad5593ee86a36c94aae2ba277c6de7d9928af768fdc52be1ce304e6d
.instagram.com	TRUE	/	FALSE	0	sessionid	46425022%3AgxJ1gyMKJIYHuM%3A1%3AAYiTBmGJRdKL3lJoh-62dkuF7a9-GkipKKFYEEa89w
.instagram.com	TRUE	/	FALSE	1731261982	wd	400x667
"""

class ExtractRequest(BaseModel):
    url: str

class YouTubeMetadataRequest(BaseModel):
    videoId: str

class DescriptionExtractionRequest(BaseModel):
    title: str
    description: str
    thumbnail: str = ""
    channelTitle: str = ""

def parse_with_ai(text: str):
    if not text.strip():
        return [], [], ""
    prompt = f"Extract recipe JSON {{ingredients: [], instructions: [], notes: \"\"}} from the following text (max 14000 chars):\n\n{text[:14000]}"
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=800
        )
        match = re.search(r"\{.*\}", resp.choices[0].message.content, re.DOTALL)
        if match:
            data = json.loads(match.group())
            return (
                data.get("ingredients", []),
                data.get("instructions", []),
                data.get("notes", "")
            )
    except:
        pass
    return [], [], ""

@app.post("/extract")
async def extract_recipe(request: ExtractRequest):
    url = request.url.strip()
    print(f"Processing: {url}")
    # 1. Regular recipe websites
    try:
        scraper = scrape_me(url)
        data = scraper.to_json()
        return {
            "title": data.get("title") or "Untitled Recipe",
            "ingredients": data.get("ingredients", []),
            "instructions": data.get("instructions", "").split("\n") if data.get("instructions") else [],
            "thumbnail": data.get("image", ""),
            "notes": "scraped with recipe-scrapers"
        }
    except:
        pass

    # 2. Video platforms (Instagram, TikTok, YouTube)
    import random
    user_agent = random.choice(USER_AGENTS)

    ydl_opts = {
        "quiet": not DEBUG_MODE,
        "no_warnings": not DEBUG_MODE,
        "verbose": DEBUG_MODE,
        "geo_bypass": True,
        "extractor_args": {
            "youtube": {
                "skip": ["hls", "dash"],
                "player_skip": ["webpage", "configs"],
            }
        },
        "sleep_interval": YT_DLP_SLEEP_INTERVAL,
        "max_sleep_interval": YT_DLP_MAX_SLEEP_INTERVAL,
        "sleep_interval_requests": 1,
        "nocheckcertificate": True,
        "prefer_insecure": False,
        "http_headers": {
            "User-Agent": user_agent,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-us,en;q=0.5",
            "Sec-Fetch-Mode": "navigate",
        },
    }
    cookie_file = None
    if INSTAGRAM_COOKIES:
        tmp = tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".txt")
        tmp.write(INSTAGRAM_COOKIES)
        tmp.close()
        cookie_file = tmp.name
        ydl_opts["cookiefile"] = cookie_file

    try:
        if DEBUG_MODE:
            print(f"[DEBUG] Extracting with user agent: {user_agent[:50]}...")

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=False)
            thumbnail = info.get("thumbnail", "")
            text = f"{info.get('description', '')}\n{info.get('title', '')}"
            ingredients, instructions, notes = parse_with_ai(text)

            if DEBUG_MODE:
                print(f"[DEBUG] Successfully extracted: {info.get('title', 'Unknown')}")

            return {
                "title": info.get("title", "Video Recipe"),
                "ingredients": ingredients,
                "instructions": instructions,
                "thumbnail": thumbnail,
                "notes": notes or "extracted from video"
            }
    except Exception as e:
        error_msg = str(e).lower()

        # Detect specific error types
        if "sign in to confirm" in error_msg or "not a bot" in error_msg:
            if DEBUG_MODE:
                print(f"[DEBUG] Bot detection triggered: {error_msg}")
            raise HTTPException(
                status_code=403,
                detail="YouTube bot detection triggered. Please try: 1) Wait 5 minutes and retry, 2) Use description paste method, or 3) Try a different video."
            )
        elif "video unavailable" in error_msg or "private video" in error_msg:
            raise HTTPException(status_code=404, detail="Video is unavailable or private")
        elif "age" in error_msg and "restrict" in error_msg:
            raise HTTPException(status_code=403, detail="Age-restricted video cannot be extracted")
        else:
            if DEBUG_MODE:
                print(f"[DEBUG] Extraction failed: {error_msg}")
            raise HTTPException(status_code=400, detail=f"Video extraction failed: {str(e)}")
    finally:
        if cookie_file and os.path.exists(cookie_file):
            os.unlink(cookie_file)

@app.post("/youtube-metadata")
async def youtube_metadata(request: YouTubeMetadataRequest):
    """Fetch YouTube video metadata using YouTube Data API"""
    video_id = request.videoId
    api_key = os.getenv("YOUTUBE_API_KEY")

    if not api_key:
        raise HTTPException(status_code=500, detail="YouTube API key not configured")

    print(f"Fetching YouTube metadata for: {video_id}")

    try:
        url = f"https://www.googleapis.com/youtube/v3/videos?part=snippet&id={video_id}&key={api_key}"
        response = requests.get(url)

        if response.status_code != 200:
            raise HTTPException(status_code=response.status_code, detail=f"YouTube API error: {response.text}")

        data = response.json()

        if not data.get("items") or len(data["items"]) == 0:
            raise HTTPException(status_code=404, detail="Video not found")

        video = data["items"][0]
        snippet = video["snippet"]

        result = {
            "title": snippet.get("title", ""),
            "description": snippet.get("description", ""),
            "thumbnail": snippet.get("thumbnails", {}).get("high", {}).get("url", "") or
                        snippet.get("thumbnails", {}).get("default", {}).get("url", ""),
            "channelTitle": snippet.get("channelTitle", "")
        }

        print(f"YouTube metadata retrieved: {result['title']}")
        return result

    except HTTPException:
        raise
    except Exception as e:
        print(f"Error fetching YouTube metadata: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch YouTube metadata: {str(e)}")

@app.post("/extract-from-description")
async def extract_from_description(request: DescriptionExtractionRequest):
    """Extract recipe from YouTube video description using GPT-4o-mini"""

    if not request.description:
        raise HTTPException(status_code=400, detail="Description is required")

    if not os.getenv("OPENAI_API_KEY"):
        raise HTTPException(status_code=500, detail="OpenAI API key not configured")

    print("Extracting recipe from description...")

    prompt = f"""Extract a structured recipe from this YouTube video description.

Video Title: {request.title or 'Unknown'}
Channel: {request.channelTitle or 'Unknown'}

Description:
{request.description}

EXTRACTION RULES:
1. Extract ALL ingredients with their measurements exactly as written
2. Extract ALL cooking instructions in order
3. Extract prep time, cook time, and servings if mentioned
4. If times aren't specified, estimate based on recipe complexity
5. Identify cuisine type and difficulty level
6. Extract any dietary tags (vegetarian, vegan, gluten-free, etc.)

RESPONSE FORMAT:
Respond with ONLY a valid JSON object (no markdown, no code blocks):
{{
  "title": "recipe title from description or video title",
  "description": "brief description of the dish",
  "prepTime": number (minutes),
  "cookTime": number (minutes),
  "servings": number,
  "difficulty": "Easy" or "Medium" or "Hard",
  "cuisineType": "type of cuisine",
  "ingredients": [
    {{"name": "ingredient name", "quantity": "amount", "unit": "unit"}}
  ],
  "instructions": ["step 1", "step 2", ...],
  "dietaryTags": ["tag1", "tag2", ...],
  "notes": "any additional notes"
}}"""

    try:
        response = client.chat.completions.create(
            model='gpt-4o-mini',
            messages=[
                {"role": "system", "content": "You are a recipe extraction expert. Extract recipes from text descriptions accurately."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.3,
        )

        recipe_data = json.loads(response.choices[0].message.content)

        print(f"Description extraction successful: {recipe_data.get('title', 'Unknown')}")

        # Format ingredients as strings
        ingredients = []
        for ing in recipe_data.get("ingredients", []):
            if isinstance(ing, str):
                ingredients.append(ing)
            else:
                parts = []
                if ing.get("quantity"):
                    parts.append(str(ing["quantity"]))
                if ing.get("unit"):
                    parts.append(ing["unit"])
                if ing.get("name"):
                    parts.append(ing["name"])
                ingredients.append(" ".join(parts).strip())

        result = {
            "title": recipe_data.get("title") or request.title,
            "description": recipe_data.get("description", ""),
            "creator": request.channelTitle or "YouTube",
            "ingredients": ingredients,
            "instructions": recipe_data.get("instructions", []),
            "prep_time": recipe_data.get("prepTime", 15),
            "cook_time": recipe_data.get("cookTime", 30),
            "servings": str(recipe_data.get("servings", 4)),
            "yield": str(recipe_data.get("servings", 4)),
            "difficulty": recipe_data.get("difficulty", "Medium"),
            "cuisineType": recipe_data.get("cuisineType", "Global"),
            "dietaryTags": recipe_data.get("dietaryTags", []),
            "thumbnail": request.thumbnail or "",
            "image": request.thumbnail or "",
            "imageUrl": request.thumbnail or "",
            "notes": recipe_data.get("notes", "Extracted from video description"),
            "extractionMethod": "description"
        }

        return result

    except Exception as e:
        print(f"Error extracting from description: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to extract recipe from description: {str(e)}")

@app.post("/extract-youtube-transcript")
async def extract_youtube_transcript(request: ExtractRequest):
    """Alternative extraction using YouTube transcript API (no yt-dlp)"""
    url = request.url.strip()
    video_id_match = re.search(r'(?:v=|youtu\.be/)([a-zA-Z0-9_-]{11})', url)

    if not video_id_match:
        raise HTTPException(status_code=400, detail="Invalid YouTube URL")

    video_id = video_id_match.group(1)
    api_key = os.getenv("YOUTUBE_API_KEY")

    if not api_key:
        raise HTTPException(status_code=500, detail="YouTube API key not configured")

    try:
        # Get video metadata
        metadata_url = f"https://www.googleapis.com/youtube/v3/videos?part=snippet&id={video_id}&key={api_key}"
        metadata_response = requests.get(metadata_url)

        if metadata_response.status_code != 200:
            raise HTTPException(status_code=metadata_response.status_code, detail="Failed to fetch video metadata")

        metadata = metadata_response.json()
        if not metadata.get("items"):
            raise HTTPException(status_code=404, detail="Video not found")

        snippet = metadata["items"][0]["snippet"]
        title = snippet.get("title", "YouTube Recipe")
        description = snippet.get("description", "")
        thumbnail = snippet.get("thumbnails", {}).get("high", {}).get("url", "")

        # Try to get captions/transcript
        captions_url = f"https://www.googleapis.com/youtube/v3/captions?part=snippet&videoId={video_id}&key={api_key}"
        captions_response = requests.get(captions_url)

        transcript_text = ""
        if captions_response.status_code == 200:
            captions_data = captions_response.json()
            # Note: Actual transcript download requires OAuth, so we'll use description
            if DEBUG_MODE:
                print(f"[DEBUG] Captions available but need OAuth for download")

        # Use description + title for extraction
        text = f"{description}\n{title}"
        ingredients, instructions, notes = parse_with_ai(text)

        return {
            "title": title,
            "ingredients": ingredients,
            "instructions": instructions,
            "thumbnail": thumbnail,
            "notes": notes or "Extracted from video description",
            "extractionMethod": "transcript"
        }

    except HTTPException:
        raise
    except Exception as e:
        if DEBUG_MODE:
            print(f"[DEBUG] Transcript extraction failed: {e}")
        raise HTTPException(status_code=500, detail=f"Transcript extraction failed: {str(e)}")

@app.post("/extract-manual-description")
async def extract_manual_description(request: DescriptionExtractionRequest):
    """Extract recipe from manually pasted YouTube description (no API calls)"""
    if not request.description:
        raise HTTPException(status_code=400, detail="Description is required")

    if DEBUG_MODE:
        print(f"[DEBUG] Manual description extraction for: {request.title or 'Unknown'}")

    # Use the existing description extraction logic
    return await extract_from_description(request)

@app.get("/health")
async def health_check():
    """Health check endpoint with diagnostics"""
    checks = {
        "status": "ok",
        "version": "2025-11-29-enhanced",
        "timestamp": __import__("datetime").datetime.utcnow().isoformat(),
        "youtube_api_configured": bool(os.getenv("YOUTUBE_API_KEY")),
        "openai_api_configured": bool(os.getenv("OPENAI_API_KEY")),
        "debug_mode": DEBUG_MODE,
        "yt_dlp_sleep_interval": YT_DLP_SLEEP_INTERVAL,
    }

    # Test YouTube API connectivity
    if os.getenv("YOUTUBE_API_KEY"):
        try:
            test_url = f"https://www.googleapis.com/youtube/v3/videos?part=snippet&id=dQw4w9WgXcQ&key={os.getenv('YOUTUBE_API_KEY')}"
            test_response = requests.get(test_url, timeout=5)
            checks["youtube_api_status"] = "ok" if test_response.status_code == 200 else f"error_{test_response.status_code}"
        except Exception as e:
            checks["youtube_api_status"] = f"error: {str(e)}"

    return checks

@app.head("/")
@app.get("/")
async def root():
    return {"status": "ok", "version": "2025-11-29-enhanced", "endpoints": ["/extract", "/youtube-metadata", "/extract-from-description", "/extract-youtube-transcript", "/extract-manual-description", "/health"]}